---
title: "Data Management Group Assignment (IB9HP0)"
format: pdf
editor: visual
author: "Group 2"
---

### Introduction

This report simulates the end-to-end data management for an e-commerce platform in US from database design, workflow management to data analysis that provides insights into the platform's key performance metrics.

### Overview

![](flowchart.png)

This is the entire process of project, it begins with the database design phase, which includes conceptual, logical, and physical design. During conceptual design, we translate our understanding of the e-commerce database into an E-R. In the logical design phase, we define the structure of the database. For the physical design phase, we create the physical schema.

After designing the database, we perform the initial load of generated synthetic data and import it into the database tables, while also conducting validation check. Subsequently, we execute queries to retrieve, manipulate, and manage the data stored in the database. Finally, we conduct data analysis on our e-commerce data.

In the incremental load process, new synthetic data is generated and subjected to validation checks. If valid, the corresponding database tables and analysis graphs are updated accordingly.

### Part 1: Database Design and Implementation

#### Task 1.1: E-R Diagram Design

**Conceptual Design**

![ER Diagram](ER%20Diagram.png)

![Entity Relationship Sets](Entity%20Relationship%20Sets.png)

Our database conceptual design is based on the following assumptions & justification:

-   Customers are unique and identified by their ID. Each customer must have their first name, last name and email address whereas can opt to provide phone number or date of birth. For simplification we assume no multivalued attributes; thus, only one billing address, current shipping address and current payment method are captured in the database.
-   Products are organised into unique categories, which are identified by category ID. Each product belongs to only one category whilst one category can include multiple products, forming the 1:N relationship between Categories and Products entity.
-   Identical items sold by different sellers are considered different products and have unique product ID. Thus, one product can have only one seller but one seller may distribute multiple products, forming the 1:N relationship between Sellers and Products entity. This conceptual design is important to track price and inventory for each product sold by each seller. Additionally, each product can also have name, color, size and brand.
-   Sellers are unique and identified by their ID. We capture sellers' email, name and address.
-   Advertisements (unique and identified by ID) are specific for each product but one product can have multiple advertisements, forming the 1:N relationship between Products and Advertisements entity. Each advertisement must specify budget and captures content and the number of ad clicks.
-   Multiple customers can order multiple products that will be delivered to them by a selection of shippers (e.g., depending on the warehouse location), forming the ternary M:N relationship among the three entities. Each time a customer commits an order of a specific product which will be delivered by an assigned shipper, a unique order ID will be generated.
-   Each order captures quantity, date and time, and discount rate applicable specifically to the order. A customer can only leave a review on a product if he/she has ordered it. Thus, rating_reviews must be specific to an order, forming an attribute of the Orders relationship.
-   None of derived attributes are included to ensure physical schema comprise of only atomic values thus in normalized form.

**Logical Design**

-   **Customers** ($\underline{id}$, date_of_birth, first_name, last_name, phone_number, email, billing_address_street_number, billing_address_street_name, billing_address_street_suffix, billing_address_city, billing_address_state, billing_address_country, billing_address_postcode, current_shipping_address_street_number, current_shipping_address_street_name, current_shipping_address_street_suffix, current_shipping_address_city, current_shipping_address_state, current_shipping_address_country, current_shipping_address_postcode, current_payment_method)
-   **Shippers** ($\underline{id}$, name, phone_number)
-   **Products** ($\underline{id}$, $\underline{\underline{seller\_id}}$, $\underline{\underline{category\_id}}$, name, color, size, brand, price, currency, inventory)
-   **Order** ($\underline{id}$, $\underline{\underline{customer\_id}}$ , $\underline{\underline{product\_id}}$ , $\underline{\underline{shipper\_id}}$[,]{.underline} order_date, order_time, quantity, discount, rating_review)
-   **Seller** ($\underline{id}$, name, email, address_street_number, address_street_name, address_street_suffix, address_city, address_state, address_country, address_postcode)
-   **Advertisements** ($\underline{id}$, $\underline{\underline{product\_id}}$[,]{.underline} content, ad_clicks, budget, currency)
-   **Categories** ($\underline{id}$[,]{.underline} name, description)

Our logical design of the database is converted from the conceptual ER diagram on the following principles:

-   Primary keys are underlined whereas foreign keys are double underlined.

-   The primary key of the weak side become the foreign key on the strong side of the cardinality.

-   Composite attributes (customer name, customer and seller address, product price and advertisement budget ) are registered in individual fields using outer layer of the attribute to ensure that physical database only capture atomic value.

-   Each entity is converted into one table. Except for the Orders relationship, all other cardinality relationship are 1:N thus not converted into table.

-   The ternary M:N relationship among Customers, Products and Shippers is converted into the Orders table. Primary key (ID) of Customers, Entities and Shippers entity, together with order date and time forming a composite key for the Orders relationship table. Each record of the Orders relationship is also unique by an order ID.

#### Task 1.2: SQL Database Schema Creation

The physical schema is in First Normal Form (1NF) as:

-   Each column of a table is a unique variable and contains only atomic values (due to no composite or multivalued attribute in the logical schema).

-   Each row of a table is a unique record and identified by value in its ID attribute.

-   The order of rows and columns is not significant.

The physical schema is in Second Normal Form (2NF) and Third Normal Form (3NF) as each table are in 1NF and there are no undesirable functional dependency among the tables i.e., there is no non-prime attribute is directly or transitively partially dependent on a candidate key.

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE,comment=NA,attr.source='.numberLines')

rm(list=ls())
library(readr)
library(RSQLite)
library(dplyr)
library(ggplot2)
library(lubridate)
library(treemapify)

```

```{r connection}
# 1. set up the connection
my_db <- RSQLite::dbConnect(RSQLite::SQLite(),"../database/ecommerce.db")

```

```{bash schema, eval=FALSE}
# 2. link to SQL file to write the schema
sqlite3 "../database/ecommerce.db" < "../main/ecommerce.sql"
```

The code in **ecommerse.sql** file are shown below:

```{sql connection=my_db, eval=FALSE}
DROP TABLE IF EXISTS `customers` ;
DROP TABLE IF EXISTS `products` ;
DROP TABLE IF EXISTS `shippers` ;
DROP TABLE IF EXISTS `orders` ;
DROP TABLE IF EXISTS `advertisements` ;
DROP TABLE IF EXISTS `sellers` ;
DROP TABLE IF EXISTS `categories` ;
```

The table creation sequence starts with tables that do not have foreign keys.

```{sql connection=my_db, eval=FALSE}

------------- CREATE TABLE -------------

-- Customer Schema
CREATE TABLE IF NOT EXISTS `customers` (
  'id' INT PRIMARY KEY,
  'first_name' VARCHAR(250) NOT NULL,
  'last_name' VARCHAR(250) NOT NULL,
  'email' TEXT NOT NULL,
  'phone_number' VARCHAR(20),
  'date_of_birth' DATE,
  'billing_address_street_number' TEXT,
  'billing_address_street_name' TEXT,
  'billing_address_street_suffix' TEXT,
  'billing_address_city' TEXT,
  'billing_address_state' TEXT,
  'billing_address_country' TEXT,
  'billing_address_postcode' TEXT,
  'current_shipping_address_street_number' TEXT,
  'current_shipping_address_street_name' TEXT,
  'current_shipping_address_street_suffix' TEXT,
  'current_shipping_address_city' TEXT,
  'current_shipping_address_state' TEXT,
  'current_shipping_address_country' TEXT,
  'current_shipping_address_postcode' TEXT,
  'current_payment_method' VARCHAR(250)
);

-- Sellers Schema
CREATE TABLE IF NOT EXISTS `sellers` (
  'id' INT PRIMARY KEY,
  'name' VARCHAR(250) NOT NULL,
  'email' TEXT,
  'address_street_number' TEXT,
  'address_street_name' TEXT,
  'address_street_suffix' TEXT,
  'address_city' TEXT,
  'address_state' TEXT,
  'address_country' TEXT,
  'address_postcode' TEXT
);

-- Categories Schema
CREATE TABLE IF NOT EXISTS `categories` (
  'id' INT PRIMARY KEY,
  'name' VARCHAR(250) NOT NULL,
  'description' TEXT
);

-- Products Schema
CREATE TABLE IF NOT EXISTS `products` (
  'id' INT PRIMARY KEY,
  'seller_id' INT NOT NULL,
  'category_id' INT NOT NULL,
  'name' VARCHAR(60) NOT NULL,
  'color' VARCHAR(60) NOT NULL,
  'size' VARCHAR(5),
  'brand' VARCHAR(250),
  'price' NUMERIC NOT NULL,
  'currency' CHAR(3) NOT NULL, 
  'inventory' INT NOT NULL,
  FOREIGN KEY ('seller_id') 
    REFERENCES sellers ('id'),
  FOREIGN KEY ('category_id') 
    REFERENCES categories ('id')
);

-- Shipper Schema
CREATE TABLE IF NOT EXISTS `shippers` (
  'id' INT PRIMARY KEY,
  'name' CHAR(25) NOT NULL,
  'phone_number' VARCHAR(25) NOT NULL
);

-- Order Schema : create after 3 main tables
CREATE TABLE IF NOT EXISTS `orders` (
  'id' INT PRIMARY KEY,
  'customer_id' INT NOT NULL,
  'product_id' INT NOT NULL,
  'shipper_id' INT NOT NULL,
  'order_date' DATE NOT NULL,
  'order_time' TIMESTAMP NOT NULL,
  'quantity' INT NOT NULL,
  'discount' DECIMAL(3,2) NOT NULL,
  'rating_review' INT,
  FOREIGN KEY ('customer_id')
    REFERENCES customers ('id'),
  FOREIGN KEY ('product_id')
    REFERENCES products ('id'),
  FOREIGN KEY ('shipper_id')
    REFERENCES shippers ('id')
);

-- Ads Schema
CREATE TABLE IF NOT EXISTS `advertisements` (
  'id' INT PRIMARY KEY,
  'product_id' INT NOT NULL,
  'content' TEXT,
  'ad_clicks' INT,
  'budget' DECIMAL(10,2),
  'currency' CHAR(3),
  FOREIGN KEY ('product_id')
	REFERENCES products ('id')
);
```

#### Part 2: Data Generation and Management

#### Task 2.1: Synthetic Data Generation

The entire synthetic data is generated using Mockaroo using advanced field settings or Mockaroo specified coding language.

Foreign key is imported from other datasets by using field type as 'Dataset Column' then connecting to the other datasets where the foreign key comes from.

Field type as 'Formula' or 'Custom List' with dynamic distribution are utilised to set specific rules and conditions for the synthetic data generation as realistic as possible, for example:

-   Product names are conditional to category ID

-   Product size and price ranges are conditional to product name

-   Advertisement contents are conditional to product name

The data generation is a part of the dynamic process from data generation to data validation then data analysis. Data generation is revised whenever challenges incur during validation and analysis to ensure that the synthetic data simulates the real-world as much as possible.

#### Task 2.2: Data Import and Quality Assurance

In this task, we have performed data validation for entities in the database to maintain data quality and integrity. The validation process involves several key activities:

**1. Checking Duplicate Primary Key:** First, we check whether any of the primary keys present in the csv files have duplicates or not. If the number of unique values for a specific primary key column is equal to the total number of rows in the file, then there would be no duplicate primary key values. If there are duplicate entries, then an error message would pop up and the whole process would stop.

**2. Checking data quality and integrity:**

**2.1. Consistency Checks:** Consistency checks involve ensuring that the data conforms to business rules and logic. For instance, a payment status should correspond to a limited set of values (e.g., 'Pending', 'Completed', 'Rejected'), and a product category should be one that is predefined in the category table.

**2.2. Range & Constraint Checks:** This involves ensuring that numeric values fall within acceptable ranges (e.g., a rating field that should be between 1 and 5) and that string values meet constraints, such as a maximum length.

**2.3. Email Validation:** Email addresses are validated using regular expressions that define a pattern consistent with standard email formats. The validation ensures that only correctly formatted email addresses are accepted, which is critical for communication and user identification.

**2.4. Phone Numbers:** Phone numbers are validated using numeric values. The first digit for all phone numbers starts with "1" for country code, followed by 10 numeric values. However, we do not cover the international format and only consider phone numbers from the USA. This is checked in both "customers" and "shippers" entities.

**2.5. Date:** Order dates, present in the "order" entity, are validated using the YYYY-MM-DD format. The parser interprets the characters in the date according to this format and checks if they follow it. If the dates are valid, the system would indicate **'TRUE'**. If the dates are invalid, the system would indicate **'FALSE'**.

**2.6. Currency Code:** For the currency code, we assume that only USD is used. By standardising on a single currency code, the database is less complex and more consistent. This would also make sure the fluctuations in currency would not affect the change in prices.

**2.7. Price, Quantity, Budget & Ad Clicks:** Price from the "products" entity, quantity from the "orders" entity, budget and ad clicks from the "advertisements" entity are validated using an inequality, which states that these 3 attributes must be greater than 0 for their respective entities.

**2.8. Payment Method:** Payment method, present in the "customers" entity, is validated by checking whether an appropriate payment method is provided by the customer. This column cannot have null values.

**2.9. Discount:** Discount, present in the "orders" entity, is validated by checking whether each value in the column is between 0 to 100.

**2.10. Rating Review:** Rating review, present in the "orders" entity, is validated by checking whether each value in the column is either NA or a value from 1 to 5.

If any of the values present in the above attributes go against the set rules, a warning statement would pop up, indicating that specific row should be excluded.

**3. Checking Foreign Key**: After performing the integrity check, we check the foreign key. First, we check if there is/are foreign key(s) present in the file. If there are no foreign keys, the validation is skipped. If they are present in the file, the foreign key columns are extracted to a table for temporary processing. In this manner, all the csv files are checked. The foreign key values from the destination columns are, then, checked to see if they exist in the original columns. In a separate column, if the foreign key is present, the validation is set to **'TRUE'** and if the foreign key is missing, the validation is set to **'FALSE'**. The rows indicating 'FALSE' would be removed and the file would be updated.

The validation is, thus, completed.

The validations are stored in **Validation.R** file are shown below:

```{r validation, eval=FALSE}
print("Performing Validation")

# Get the primary key column names from the database
query <- paste("SELECT name FROM pragma_table_info('",table_name,"') WHERE pk = 1;",sep="")
primary_key_columns <- dbGetQuery(my_db, query)

# Get Foreign Key
query <- paste("PRAGMA foreign_key_list('",table_name,"');",sep="")
foreign_key_columns <- dbGetQuery(my_db, query)

# ------ 1. Check duplicate primary key within CSV file ------
print(paste0("Checking duplicate primary key for: ",variable))

number_of_rows <- nrow(this_file_contents)

for (i in primary_key_columns) {
  if (nrow(unique(this_file_contents[,i]))==number_of_rows) {
    print(paste("Primary key =",i,": Passed"))
  }
  else {
    stop(paste("Found duplicate record in ", variable,": STOP process!"))
  }
}


# ------ 2. Check data quality and integrity ------
print(paste0("Checking integrity for: ",variable))

# Function to validate email addresses
validate_emails <- function(emails) {
  pattern <- "^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$"
  grepl(pattern, emails)
}

# Function to validate phone numbers
validate_phones <- function(phones) {
  # This is a simple example and might not cover all international formats
  pattern <- "^1[0-9]{10}$$"
  grepl(pattern, phones)
}

# Function to validate dates
validate_dates <- function(dates) {
  date_format <- "%Y-%m-%d"
  dates_parsed <- parse_date_time(dates, orders = date_format)
  !is.na(dates_parsed)
}

# Function to validate prices
validate_prices <- function(prices) {
  prices > 0
}

# Function to validate currency codes
validate_currencies <- function(currencies) {
  pattern <- "^USD$"
  grepl(pattern, currencies)
}

# Function to validate payment method
validate_payment_method <- function(payment_method){
  valid_method <- c("Apple Pay", "Google Pay", "Credit Card", "Cash", "Debit Card", "Bank Transfer", "Cheque")
  payment_method %in% valid_method
}

# Function to validate ad clicks
validate_ad_clicks <- function(ad_clicks) {
  ad_clicks >= 0
}

# Function to validate discount
validate_discount <- function(discount) {
  (discount >= 0 & discount <= 100)
}

# Function to validate rating_review
validate_rating_review <- function(rating_review) {
  valid_rating <- c(1, 2, 3, 4, 5)
  is.na(rating_review) | rating_review %in% valid_rating
}

# Function error handling
validation <- function(this_file_contents,type,column) {
  tmp_table <- this_file_contents
  if (type == 'Email') {
    tmp_table$valid_format <- validate_emails(column)
  } else if (type == 'Phone_numbers') {
    tmp_table$valid_format <- validate_phones(column)
  } else if (type == 'Dates') {
    tmp_table$valid_format <- validate_dates(column)
  } else if (type == 'Prices' || type == 'Budget' || type == 'Quantity') {
    tmp_table$valid_format <- validate_prices(column)
  } else if (type == 'Currencies') {
    tmp_table$valid_format <- validate_currencies(column)
  }
  else if (type == 'payment_method') {
    tmp_table$valid_format <- validate_payment_method(column)
  }
  else if (type == 'ad_clicks') {
    tmp_table$valid_format <- validate_ad_clicks(column)
  }
  else if (type == 'discount') {
    tmp_table$valid_format <- validate_discount(column)
  }
  else if (type == 'rating_review') {
    tmp_table$valid_format <- validate_rating_review(column)
  }
  if (nrow(tmp_table) >0) {
    for (i in 1:nrow(tmp_table)){
      tmp_row <- tmp_table[i,]
      if (!tmp_row$valid_format) {
        warning(type," Format of ID: ",tmp_row$id, " in ", variable," is incorrect. Please check." )
      }
    }
  }
  if (all(tmp_table$valid_format) == TRUE){
    print(paste(type," Format: Passed!"))
  }
  tmp_table <- tmp_table[tmp_table$valid_format,] # remove row
  tmp_table <- tmp_table[, !names(tmp_table) %in% "valid_format"] # remove check column
  return(tmp_table)
}

# Perform integrity check
if (table_name == 'customers' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Email',this_file_contents$email)
  this_file_contents <- validation(this_file_contents,'Phone_numbers',this_file_contents$phone_number)
  this_file_contents <- validation(this_file_contents,'payment_method',this_file_contents$current_payment_method)
  
} else if (table_name == 'orders' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Dates',this_file_contents$order_date)
  this_file_contents <- validation(this_file_contents,'discount',this_file_contents$discount)
  this_file_contents <- validation(this_file_contents,'Quantity',this_file_contents$quantity)
  this_file_contents <- validation(this_file_contents,'rating_review',this_file_contents$rating_review)
} else if (table_name == 'products' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Prices',this_file_contents$price)
  this_file_contents <- validation(this_file_contents,'Currencies',this_file_contents$currency)
  
} else if (table_name == 'categories' && nrow(this_file_contents) >0) {
  
} else if (table_name == 'sellers' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Email',this_file_contents$email)
  
} else if (table_name == 'shippers' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Phone_numbers',this_file_contents$phone_number)
} else if (table_name == 'advertisements' && nrow(this_file_contents) >0) {
  this_file_contents <- validation(this_file_contents,'Currencies',this_file_contents$currency)
  this_file_contents <- validation(this_file_contents,'Budget',this_file_contents$budget)
  this_file_contents <- validation(this_file_contents,'ad_clicks',this_file_contents$ad_clicks)
}

# ------ 3. Check Foreign key ------
if(nrow(this_file_contents) >0) {
  foreign_table <- foreign_key_columns[,'table']
  tmp_table <- this_file_contents
  for (i in foreign_table) {
    foreign_key_ori_column <- foreign_key_columns[foreign_key_columns[,'table'] == i,'from']
    foreign_key_dest_column <- foreign_key_columns[foreign_key_columns[,'table'] == i,'to']
    print(paste("Checking Foreign key in table",i, "column:",foreign_key_ori_column))
    for (j in 1:nrow(this_file_contents)) {
      foreign_key_value <- this_file_contents[j,foreign_key_ori_column]
      query <- paste("SELECT", foreign_key_dest_column," FROM",i," WHERE", foreign_key_dest_column,"=", foreign_key_value, ";")
      result <- dbGetQuery(my_db, query)
      col <- paste("check_",i,sep="")
      if (nrow(result) == 0) {
        warning("Foreign key is missing in row ID =  ", this_file_contents[j,primary_key_columns[1,]], " Please check.")
        tmp_table[[col]] <- FALSE
      } else {
        tmp_table[[col]] <- TRUE
      }
    }
  }
  rows_to_remove <- apply(tmp_table[, grepl("^check", names(tmp_table))], 1, function(row) any(!row))
  tmp_table <- tmp_table[, !grepl("^check", names(tmp_table))] # remove column
  tmp_table <- tmp_table[!rows_to_remove, ] # remove failed row
  this_file_contents <- tmp_table
} else{
  print("No validation check in this table since there's no foreign key")
}

print("Validation Completed")

```

The initial load has been performed.

```{r initial_load}
# Get only Initial file which has the format [table_name].csv
all_files <- setdiff(list.files("../data_upload/"), list.files("../data_upload/", pattern = "_"))
# Order the files to load to database, to avoid error from foreign key
custom_order <- list("customers.csv","sellers.csv","categories.csv","products.csv","shippers.csv","orders.csv","advertisements.csv")
all_files <- all_files[order(match(all_files, custom_order))]

for (variable in all_files) {
  this_filepath <- paste0("../data_upload/",variable)
  this_file_contents <- readr::read_csv(this_filepath)

  table_name <- gsub(".csv","",variable)
  
  # Perform Validation
  source("../main/Validation.R")
  
  # convert column date format
  if (table_name == 'orders') {
    this_file_contents['order_date'] <- lapply(this_file_contents['order_date'], as.character)
  }
  
  if (nrow(this_file_contents)>0 ){
      for (i in 1:nrow(this_file_contents)) {
        row <- this_file_contents[i, ]
        
        # Extract primary key values from the row
        primary_key_values <- paste(names(row)[names(row) %in% primary_key_columns], row[names(row) %in% primary_key_columns], sep = "=", collapse = " AND ")
        
        # Find if the primary key exists
        query <- paste("SELECT * FROM", table_name, paste("WHERE", primary_key_values))
        existing_row <- dbGetQuery(my_db, query)
        
        if (nrow(existing_row) == 0) {
          # Row is unique, append to the table
          #print(paste("Append:",primary_key_values))
          dbWriteTable(my_db, table_name, row, append = TRUE)
        } else {
          # Row already exists, update the existing row
          #print(paste("Update:",primary_key_values))
          update_query <- paste("UPDATE", table_name, paste("SET", paste(names(row), "=", paste0("'", row, "'"), collapse = ", "), "WHERE", primary_key_values))
          dbExecute(my_db, update_query)
        }
      }
    }
    else {
      print("Nothing to update in database since all rows are not pass the validations")
    }
}
```

### Part 3: Data Pipeline Generation

#### Task 3.1: GitHub Repository and Workflow Setup

For setting up the GitHub repository and workflow, we first create a new repository for our project on GitHub, and then clone it to our local machine using Git. This enables collaboration within our group, allowing everyone to work together and make changes to the project through GitHub. After cloning the repository locally, we add our database file, e-commerce data file, report file, as well as scripts for data validation, transformation, data analysis, and data visualization to the repository.

Within the 'database' file, an e-commerce database is created, and in the 'data upload' file, we upload all the generated data, including tables: advertisements, categories, customers, products, sellers, shippers. Within the 'main' file, we have data analysis visualizations file, and transformation, validation scripts.

#### Task 3.2: GitHub Actions for Continuous Integration

To set up the workflow, we use the "actions" part to create a new workflow file, then define the workflow as **etl.yaml**. 

---
name: ETL Workflow for Group 2

on:
  schedule:
    - cron: '0 */2 * * *' # Run every 3 hours
  push:
    branches: [ main ]
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.2.0'
      - name: Cache R packages
        uses: actions/cache@v2
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-r-${{ hashFiles('**/lockfile') }}
          restore-keys: |
            ${{ runner.os }}-r-
      - name: Install packages
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          sudo apt-get install libcurl4-openssl-dev
          Rscript -e 'install.packages(c("ggplot2", "readr", "RSQLite", "dplyr", "lubridate", "curl", "gridtext", "ggfittext", "maps", "mapproj", "gridExtra", "treemapify"))'
      - name: Execute R script
        run: |
          Rscript main/Transformation.R
      - name: Add files
        run: |
          git config --global user.email "pomprodpran@hotmail.com"
          git config --global user.name "pomprodpran"
          git add --all database/
          git add --all Visualisations/
      - name: Commit files
        run: |
          git diff --quiet && git diff --staged --quiet || git commit -m "Updated Database and Visualisation"
      - name: Push changes
        uses: ad-m/github-push-action@v0.6.0
        with:
            github_token: ${{ secrets.GITHUB_TOKEN }}
            branch: main
---

For the workflow, we specify the actions as follows:

1\. Specify that the workflow should run every 2 hours or when changes are pushed to the main branch.

2\. Define and build the job (sequence of tasks) to be executed.

3\. Check out the code repository into the GitHub Actions runner.

4\. Set up the R environment and cache R packages.

5\. Install all the packages that we will use.

6\. Execute the **Transformation.R** script from the main directory, checking for data quality and integrity of the new data that we update. (After data transformation and validation, it will trigger the running of the data analysis script, and the new analysis charts will be saved by month in the folder.)

```{r incremental_load, eval=FALSE}
# Transformation.R
library(readr)
library(RSQLite)
library(dplyr)
library(ggplot2)
library(lubridate)

# Incremental Load
print("Loading CSV file")

# File format for automation: <table name>_YYYY-MM-DDTHHMMSS.csv
current_date <- Sys.Date()
print(paste("current date:", current_date))
# Get only Incremental file
all_files <- list.files("./data_upload", full.names = FALSE, pattern = "_")
for (variable in all_files) {
  file_name <- unlist(strsplit(gsub(".csv","",variable), "_")) # split file name using _ separator
  table_name <- file_name[1]
  date_time_parts <- unlist(strsplit(file_name[2], "T"))  # Splitting file name using 'T' separator
  date_str <- date_time_parts[1]  # Date string
  time_str <- date_time_parts[2]  # Time string
  date_value <- lubridate::ymd(date_str) # Parsing date strings into datetime objects using lubridate
  
  # Get only NEW file that has been loaded into the folder (and run historical back 2 days)
  if (date_value>= current_date-1 && date_value<= current_date ) {
    print(paste("Reading file:",variable))
    this_filepath <- paste0("./data_upload/",variable)
    this_file_contents <- readr::read_csv(this_filepath)
    
    print(paste("Writing table to database:", table_name))
    my_db <- RSQLite::dbConnect(RSQLite::SQLite(),"./database/ecommerce.db")
    
    # Perform Validation
    source("./main/Validation.R")
    
    # convert column date format
    if (table_name == 'orders') {
      this_file_contents['order_date'] <- lapply(this_file_contents['order_date'], as.character)
    }
    
    # Validation and Writing on each row to DB
    if (nrow(this_file_contents)>0 ){
      for (i in 1:nrow(this_file_contents)) {
        
        row <- this_file_contents[i, ]
        
        # Extract primary key values from the row
        primary_key_values <- paste(names(row)[names(row) %in% primary_key_columns], row[names(row) %in% primary_key_columns], sep = "=", collapse = " AND ")
        
        # Find if the primary key exists
        query <- paste("SELECT * FROM", table_name, paste("WHERE", primary_key_values))
        existing_row <- dbGetQuery(my_db, query)
        
        if (nrow(existing_row) == 0) {
          # Row is unique, append to the table
          print(paste("Append:",primary_key_values))
          dbWriteTable(my_db, table_name, row, append = TRUE)
        } else {
          # Row already exists, update the existing row
          print(paste("Update:",primary_key_values))
          update_query <- paste("UPDATE", table_name, paste("SET", paste(names(row), "=", paste0("'", row, "'"), collapse = ", "), "WHERE", primary_key_values))
          dbExecute(my_db, update_query)
        }
      }
    }
    else {
      print("Nothing to update in database since all rows are not pass the validations")
    }
  }
}

# Check if the connection object exists and is valid - if yes, mean there's updated data
if (exists("my_db") && RSQLite::dbIsValid(my_db)) {
  # Perform Visualisation
  source("./main/Visualisation.R")
  print("Done!")
  # Disconnect from the database
  RSQLite::dbDisconnect(my_db)
} else {
  # Print a message or handle the case where the connection object is not found or invalid
  print("Connection object not found or is invalid.")
}

```

When performing incremental loading, the new loaded CSV file format for automation needs to be '

'\[table name\]\_YYYY-MM-DDTHHMMSS.csv' to read in the data.

Only the newly added files within the past two days will be processed, and we will obtain the primary key to check for duplicate primary keys first. Then, we will validate the quality and integrity of the new data, such as validating email and phone number formats. If errors occur, the uploaded error data will be removed while providing warnings for each invalid entry.

After data validation, we use the primary key to check if the row is unique. If it is, the row will be appended to the table. If the row already exists, we will update the existing row.

7\. Configure the Git user email and name, adding all files in the database directory to the Git staging area.

8\. Finally, commit and push the changes to the main branch. 

### Part 4: Data Analysis and Reporting with Quarto in R
